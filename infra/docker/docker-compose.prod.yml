# ══════════════════════════════════════════════════════════
#  Muraho Rwanda — Production Docker Compose
#  Self-hosted on GPU server. vLLM for LLM inference.
# ══════════════════════════════════════════════════════════

services:
  # ── vLLM (Production LLM Inference) ──────────────────
  vllm:
    image: vllm/vllm-openai:latest
    container_name: muraho-vllm
    runtime: nvidia
    environment:
      NVIDIA_VISIBLE_DEVICES: all
    ports:
      - "8080:8000"
    volumes:
      - model_cache:/root/.cache/huggingface
    command: >
      --model mistralai/Mistral-7B-Instruct-v0.3
      --host 0.0.0.0
      --port 8000
      --max-model-len 8192
      --gpu-memory-utilization 0.85
      --dtype half
      --enforce-eager
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s # vLLM needs time to load models

  # ── AI Service (FastAPI) ─────────────────────────────
  ai-service:
    build:
      context: ../../ai-service
      dockerfile: Dockerfile
    container_name: muraho-ai
    environment:
      LLM_BACKEND: vllm
      LLM_BASE_URL: http://vllm:8000/v1
      LLM_MODEL_FAST: mistralai/Mistral-7B-Instruct-v0.3
      LLM_MODEL_HEAVY: mistralai/Mistral-7B-Instruct-v0.3 # Same model on Tier 1
      DATABASE_URL: postgresql://muraho:${DB_PASSWORD}@postgres:5432/muraho
      REDIS_URL: redis://redis:6379/0
      EMBEDDING_DEVICE: cpu
      EMBEDDING_MODEL: intfloat/multilingual-e5-large
      WHISPER_DEVICE: cuda
      ENABLE_AUDIT_LOG: "true"
    ports:
      - "8000:8000"
    depends_on:
      vllm:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu] # Shared GPU for Whisper

  # ── PostgreSQL + pgvector + PostGIS ──────────────────
  postgres:
    image: postgis/postgis:16-3.4
    container_name: muraho-postgres
    environment:
      POSTGRES_DB: muraho
      POSTGRES_USER: muraho
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init.sql
      - ./scripts/postgresql.conf:/etc/postgresql/postgresql.conf
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U muraho"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ── Redis ────────────────────────────────────────────
  redis:
    image: redis:7-alpine
    container_name: muraho-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ── MinIO (Media Storage) ────────────────────────────
  minio:
    image: minio/minio:latest
    container_name: muraho-minio
    environment:
      MINIO_ROOT_USER: ${MINIO_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_PASSWORD}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"

  # ── App Server (Payload CMS + Next.js) ───────────────
  app:
    build:
      context: ../../
      dockerfile: Dockerfile
    container_name: muraho-app
    environment:
      DATABASE_URI: postgresql://muraho:${DB_PASSWORD}@postgres:5432/muraho
      PAYLOAD_SECRET: ${PAYLOAD_SECRET}
      AI_SERVICE_URL: http://ai-service:8000
      REDIS_URL: redis://redis:6379/1
      MINIO_ENDPOINT: http://minio:9000
      MINIO_ACCESS_KEY: ${MINIO_USER}
      MINIO_SECRET_KEY: ${MINIO_PASSWORD}
      NEXT_PUBLIC_MAPBOX_TOKEN: ${MAPBOX_TOKEN}
    ports:
      - "3000:3000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      ai-service:
        condition: service_started

volumes:
  postgres_data:
  redis_data:
  minio_data:
  model_cache:
